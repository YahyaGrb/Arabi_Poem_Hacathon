{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YahyaGrb/Arabi_Poem_Hacathon/blob/main/AraBERT_finteune_poem_fill_mask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EifbsBYpnnv"
      },
      "outputs": [],
      "source": [
        "pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdLtT3jApvES"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, TFAutoModelForMaskedLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTJFVhSRrEMx"
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/docs/transformers/model_sharing?highlight=login#setup\n",
        "!huggingface-cli login #grant access to my private datasets\n",
        "# add the token from https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# select a pretrained model"
      ],
      "metadata": {
        "id": "nD3VNQ_0crHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint = \"aubmindlab/bert-base-arabertv02\" # classic arabic could be more close to poems\n",
        "checkpoint = \"distilbert-base-multilingual-cased\" # not very good performance with primary test\n",
        "# checkpoint = \"CAMeL-Lab/bert-base-arabic-camelbert-ca\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = TFAutoModelForMaskedLM.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "CIlN3YGIcqPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "id": "zW8USe2dds8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(model.dummy_inputs)  # Build the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "yjqK5Fo-eIpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run a quick test\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "text= \"قفا نبك من ذِكرى حبيب و [MASK]\"\n",
        "inputs = tokenizer(text, return_tensors=\"np\")\n",
        "token_logits = model(**inputs).logits\n",
        "# Find the location of [MASK] and extract its logits\n",
        "mask_token_index = np.argwhere(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "# Pick the [MASK] candidates with the highest logits\n",
        "# We negate the array before argsort to get the largest, not the smallest, logits\n",
        "top_5_tokens = np.argsort(-mask_token_logits)[:5].tolist()\n",
        "\n",
        "for token in top_5_tokens:\n",
        "    print(f\">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}\")"
      ],
      "metadata": {
        "id": "k23X4-1OdSVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIQVmvMHpvIc"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"Yah216/Poem_APCD_text_only\", use_auth_token=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIM0rRHntgaa"
      },
      "source": [
        "## Prep the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9TlAXwMxnmc"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
        "for row in sample:\n",
        "    print(f\"\\n'>>> البيت: {row['البيت']}'\")"
      ],
      "metadata": {
        "id": "giChyQ-ngON4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THsFcLkws_Qd"
      },
      "source": [
        "## tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo1sTlPes-t6"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    result = tokenizer(examples[\"البيت\"])\n",
        "    if tokenizer.is_fast:\n",
        "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
        "    return result\n",
        "\n",
        "\n",
        "# Use batched=True to activate fast multithreading!\n",
        "tokenized_datasets = dataset.map(\n",
        "    tokenize_function, batched=True, remove_columns=[\"البيت\"]\n",
        ")\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create data chunks"
      ],
      "metadata": {
        "id": "z9vemG1whXPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 128"
      ],
      "metadata": {
        "id": "OHv02TDshfoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare chunking\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    # Compute length of concatenated texts\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the last chunk if it's smaller than chunk_size\n",
        "    total_length = (total_length // chunk_size) * chunk_size\n",
        "    # Split by chunks of max_len\n",
        "    result = {\n",
        "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    # Create a new labels column\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "id": "YLgCcVgMh5PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_datasets = tokenized_datasets.map(group_texts, batched=True, num_proc = 8)\n",
        "lm_datasets"
      ],
      "metadata": {
        "id": "187hCjB8iDQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(lm_datasets[\"train\"][5][\"input_ids\"])"
      ],
      "metadata": {
        "id": "i2H8UoXniiQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data collator "
      ],
      "metadata": {
        "id": "sgkWg90liwwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 400000\n",
        "test_size = int(0.05 * train_size)\n",
        "\n",
        "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
        "    train_size=train_size, test_size=test_size, seed=42\n",
        ")\n",
        "downsampled_dataset"
      ],
      "metadata": {
        "id": "IfT-arjymVJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
      ],
      "metadata": {
        "id": "Ca9moOkjiv_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_train_dataset = downsampled_dataset[\"train\"].to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        "    batch_size=32,\n",
        ")\n",
        "\n",
        "tf_eval_dataset = downsampled_dataset[\"test\"].to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    batch_size=32,\n",
        ")"
      ],
      "metadata": {
        "id": "T4hlou5ome1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "X0JckFwvm3yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import create_optimizer\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "import tensorflow as tf\n",
        "\n",
        "num_train_steps = len(tf_train_dataset)\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5,\n",
        "    num_warmup_steps=1_000,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=0.01,\n",
        ")\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "# Train in mixed-precision float16\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "callback = PushToHubCallback(\n",
        "    \"Yah216/DistilBERT-finetuned-ACDP\", tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "vw7as3T5mlG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# eval perplexity"
      ],
      "metadata": {
        "id": "T00rXMM-pcCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "eval_loss = model.evaluate(tf_eval_dataset)\n",
        "print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"
      ],
      "metadata": {
        "id": "u9hs-HoMpbHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs = 3,\n",
        "                    callbacks=[callback]\n",
        "                    )"
      ],
      "metadata": {
        "id": "1yqkOkrwpj2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_ikReyAt2vg"
      },
      "outputs": [],
      "source": [
        "# save the checkpoint\n",
        "model.save_pretrained(\"model/DistilBERT-finetuned-ACDP\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test the model"
      ],
      "metadata": {
        "id": "0vsDfOA4wUp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text= \" إليك عني صديقي لندهب إلى بيت [MASK] \"\n",
        "inputs = tokenizer(text, return_tensors=\"np\")\n",
        "token_logits = model(**inputs).logits\n",
        "# Find the location of [MASK] and extract its logits\n",
        "mask_token_index = np.argwhere(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "# Pick the [MASK] candidates with the highest logits\n",
        "# We negate the array before argsort to get the largest, not the smallest, logits\n",
        "top_5_tokens = np.argsort(-mask_token_logits)[:5].tolist()\n",
        "\n",
        "for token in top_5_tokens:\n",
        "    print(f\">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}\")"
      ],
      "metadata": {
        "id": "GpJWWnWTvVsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qY8NoKAmwiB3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AraBERT finteune poem fill mask.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNj5TSxB+j/BItT26j2VjNL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}